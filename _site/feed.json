{
    "version": "https://jsonfeed.org/version/1",
    "title": "Temiz.ai Research Blog",
    "home_page_url": "http://localhost:4000/",
    "feed_url": "http://localhost:4000/feed.json",
    "description": "Temiz.ai - Research blog of Huseyin Temiz.",
    "icon": "http://localhost:4000/assets/images/apple-touch-icon.png",
    "favicon": "http://localhost:4000/assets/images/favicon.png",
    "expired": false,
    
    "author":  {
        "name": "Hüseyin Temiz",
        "url": null,
        "avatar": null
    },
    
"items": [
    
        {
            "id": "http://localhost:4000/3d-gaussian-splats-avatars-utilizing-uv-mapping",
            "title": "3d Gaussian Splats Avatars Utilizing Uv Mapping",
            "summary": null,
            "content_text": "In this blog post, we examine how four recent methods utilize UV mapping within Gaussian Splat avatar pipelines. Many modern approaches convert the 3D mesh representation into a 2D UV-mapped domain to address the limitations of working directly with 3D meshes. By leveraging UV mapping, these pipelines achieve more uniform sampling, intuitive 2D editing, and efficient data compression—advantages not easily attainable with raw 3D geometry. Each method in the literature integrates UV mapping differently to maximize these benefits. Here, we explore and compare their strategies.UV Mapping in Each Method            Method      How UV Is Used                  FlashAvatar [1]      • Canonical field initialization by sampling Gaussians uniformly over the UV map yields a far more even distribution across face, hair, and neck regions.• Mesh-driven motion uses UV→barycentric mapping to attach Gaussians to mesh and apply dynamic offsets.                  FATE   [2]      • Sampling-based densification ensures optimal Gaussian placement by sampling uniformly in UV space.• Neural baking converts discrete Gaussians into continuous attribute maps (position, color, opacity) in UV, enabling intuitive texture-style editing of the splats.                  GEM    [3]      • Gaussian eigenbasis construction: a UNet predicts multi-channel Gaussian parameter maps in the FLAME UV domain, where each pixel encodes one splat’s full parameters.• Linear distillation builds compact eigenbases over these UV maps for real-time synthesis.                  MeGA  [4]      • UV displacement map driven by FLAME parameters captures per-vertex geometric detail.• Neural textures (diffuse, view-dependent, dynamic) live in UV space for high-fidelity appearance.• Deferred rendering rasterizes the refined UV mesh + textures before blending with Gaussian hair.      Similarities &amp; DifferencesUse of UV SpaceAll four methods decouple 3D Gaussian placement and attribute editing by projecting onto a shared 2D UV domain, avoiding the irregular sampling and distortion issues of raw mesh faces.      Sampling vs. Baking          FATE &amp; FlashAvatar perform direct UV sampling to place Gaussians; FATE further bakes trained splats into continuous UV maps for editing.      GEM uses UV as an intermediate training representation, distilling it into linear eigenbases, whereas MeGA uses UV both for geometry refinement (displacement) and texture creation.      Mesh DependenceExcept for pure mesh-driven strategies, all UV-based pipelines still rely on a parametric mesh (usually FLAME) to establish UV→3D correspondences and drive animation via blendshapes or skinning.      Attachment Strategies:          FlashAvatar attaches splats directly to mesh vertices (via UV → barycentric weights) then learns offsets.      FATE samples points purely in UV and reconstructs their 3D positions via barycentric projection.      GEM requires the mesh only at inference to deform distilled eigen-coefficients.      MeGA hybridizes: the mesh handles global facial motion, UV displacement refines geometry, and Gaussian splats model hair.      Editing WorkflowUV domains unlock powerful 2D editing: painting, diffusion-model edits, or neural filters—far more intuitive than manipulating unstructured 3D point clouds.      Continuous vs. Discrete:          FATE creates continuous UV attribute maps, enabling pixel-space edits.      GEM keeps discrete splats but trains via UV maps for compactness.      MeGA splits geometry (UV displacement) from appearance (neural textures), offering fine-grained control in both domains.      Advantages of UV Mapping over Direct Mesh Sampling      Uniform Density ControlUV sampling lets you enforce an even coverage of splats (e.g., FlashAvatar), sidestepping the uneven vertex spacing or face-size distortions of direct 3D sampling.        2D Editing ToolchainAttribute maps in UV space can be edited with mature 2D tools—painting, diffusion models, or neural filters—making appearance tweaks as straightforward as texture work on polygonal models.        Compact Representation &amp; AccelerationUV maps lend themselves to image-based compression or CNN/UNet processing (as in GEM), enabling low-dimensional bases and lightning-fast inference on commodity hardware.        Decoupling Geometry &amp; AppearanceBy separating UV displacement for shape from UV textures for color and dynamics (MeGA), pipelines simplify rendering and editing complexity.        Stable Barycentric MappingSampling in UV then projecting via barycentric coordinates (FATE) avoids mesh discretization artifacts and ensures smooth, differentiable optimization of Gaussian parameters.  ConclusionIn sum, UV parameterizations transform complex 3D Gaussian splat operations into well-behaved 2D workflows, yielding more uniform sampling, intuitive editing, and powerful opportunities for compression and acceleration that mesh-only approaches simply cannot match.References[1] FlashAvatar: Xiang, Jun, et al. “Flashavatar: High-fidelity head avatar with efficient gaussian embedding.” CVPR 2024.[3] FATE: Zhang, Jiawei, et al. “Fate: Full-head gaussian avatar with textural editing from monocular video.” CVPR 2025.[4] GEM: Zielonka, Wojciech, et al. “Gaussian eigen models for human heads.” CVPR 2025.[2] MEGA: Wang, Cong, et al. “Mega: Hybrid mesh-gaussian head avatar for high-fidelity rendering and head editing.” CVPR 2025.",
            "content_html": "<p>In this blog post, we examine how four recent methods utilize UV mapping within Gaussian Splat avatar pipelines. Many modern approaches convert the 3D mesh representation into a 2D UV-mapped domain to address the limitations of working directly with 3D meshes. By leveraging UV mapping, these pipelines achieve more uniform sampling, intuitive 2D editing, and efficient data compression—advantages not easily attainable with raw 3D geometry. Each method in the literature integrates UV mapping differently to maximize these benefits. Here, we explore and compare their strategies.</p><h3 id=\"uv-mapping-in-each-method\">UV Mapping in Each Method</h3><table>  <thead>    <tr>      <th>Method</th>      <th>How UV Is Used</th>    </tr>  </thead>  <tbody>    <tr>      <td><strong>FlashAvatar</strong> [1]</td>      <td>• <strong>Canonical field initialization</strong> by sampling Gaussians uniformly over the UV map yields a far more even distribution across face, hair, and neck regions.<br />• <strong>Mesh-driven motion</strong> uses UV→barycentric mapping to attach Gaussians to mesh and apply dynamic offsets.</td>    </tr>  </tbody>  <tbody>    <tr>      <td><strong>FATE</strong>   [2]</td>      <td>• <strong>Sampling-based densification</strong> ensures optimal Gaussian placement by sampling uniformly in UV space.<br />• <strong>Neural baking</strong> converts discrete Gaussians into continuous attribute maps (position, color, opacity) in UV, enabling intuitive texture-style editing of the splats.</td>    </tr>  </tbody>  <tbody>    <tr>      <td><strong>GEM</strong>    [3]</td>      <td>• <strong>Gaussian eigenbasis construction</strong>: a UNet predicts multi-channel Gaussian parameter maps in the FLAME UV domain, where each pixel encodes one splat’s full parameters.<br />• <strong>Linear distillation</strong> builds compact eigenbases over these UV maps for real-time synthesis.</td>    </tr>  </tbody>  <tbody>    <tr>      <td><strong>MeGA</strong>  [4]</td>      <td>• <strong>UV displacement map</strong> driven by FLAME parameters captures per-vertex geometric detail.<br />• <strong>Neural textures</strong> (diffuse, view-dependent, dynamic) live in UV space for high-fidelity appearance.<br />• <strong>Deferred rendering</strong> rasterizes the refined UV mesh + textures before blending with Gaussian hair.</td>    </tr>  </tbody></table><h3 id=\"similarities--differences\">Similarities &amp; Differences</h3><!-- | Aspect | FATE | FlashAvatar | GEM | MeGA ||--------|------|-------------|-----|------|| **UV Usage** | Sampling + Neural baking | Canonical field initialization | Gaussian eigenbasis construction | UV displacement + Neural textures || **Attachment** | UV sampling → barycentric | Direct mesh attachment + offsets | Mesh-independent eigen-coefficients | Hybrid: mesh + UV + Gaussians || **Editing** | Continuous UV attribute maps | Discrete splat manipulation | Compact eigenbasis control | Separate geometry/appearance control || **Real-time** | Post-training baking | Direct optimization | Linear distillation | Deferred rendering pipeline | --><h4 id=\"use-of-uv-space\">Use of UV Space</h4><p>All four methods decouple 3D Gaussian placement and attribute editing by projecting onto a shared 2D UV domain, avoiding the irregular sampling and distortion issues of raw mesh faces.</p><ul>  <li>    <p><strong>Sampling vs. Baking</strong></p>    <ul>      <li>FATE &amp; FlashAvatar perform <strong>direct UV sampling</strong> to place Gaussians; FATE further <strong>bakes</strong> trained splats into continuous UV maps for editing.</li>      <li>GEM uses UV as an <strong>intermediate training representation</strong>, distilling it into linear eigenbases, whereas MeGA uses UV both for <strong>geometry refinement</strong> (displacement) and <strong>texture</strong> creation.</li>    </ul>  </li></ul><h4 id=\"mesh-dependence\">Mesh Dependence</h4><p>Except for pure mesh-driven strategies, all UV-based pipelines still rely on a parametric mesh (usually FLAME) to establish UV→3D correspondences and drive animation via blendshapes or skinning.</p><ul>  <li>    <p><strong>Attachment Strategies:</strong></p>    <ul>      <li><strong>FlashAvatar</strong> attaches splats directly to mesh vertices (via UV → barycentric weights) then learns offsets.</li>      <li><strong>FATE</strong> samples points purely in UV and reconstructs their 3D positions via barycentric projection.</li>      <li><strong>GEM</strong> requires the mesh only at inference to deform distilled eigen-coefficients.</li>      <li><strong>MeGA</strong> hybridizes: the mesh handles global facial motion, UV displacement refines geometry, and Gaussian splats model hair.</li>    </ul>  </li></ul><h4 id=\"editing-workflow\">Editing Workflow</h4><p>UV domains unlock powerful 2D editing: painting, diffusion-model edits, or neural filters—far more intuitive than manipulating unstructured 3D point clouds.</p><ul>  <li>    <p><strong>Continuous vs. Discrete:</strong></p>    <ul>      <li><strong>FATE</strong> creates <strong>continuous UV attribute maps</strong>, enabling pixel-space edits.</li>      <li><strong>GEM</strong> keeps <strong>discrete</strong> splats but trains via UV maps for compactness.</li>      <li><strong>MeGA</strong> splits <strong>geometry</strong> (UV displacement) from <strong>appearance</strong> (neural textures), offering fine-grained control in both domains.</li>    </ul>  </li></ul><h3 id=\"advantages-of-uv-mapping-over-direct-mesh-sampling\">Advantages of UV Mapping over Direct Mesh Sampling</h3><ol>  <li>    <p><strong>Uniform Density Control</strong>UV sampling lets you enforce an even coverage of splats (e.g., FlashAvatar), sidestepping the uneven vertex spacing or face-size distortions of direct 3D sampling.</p>  </li>  <li>    <p><strong>2D Editing Toolchain</strong>Attribute maps in UV space can be edited with mature 2D tools—painting, diffusion models, or neural filters—making appearance tweaks as straightforward as texture work on polygonal models.</p>  </li>  <li>    <p><strong>Compact Representation &amp; Acceleration</strong>UV maps lend themselves to image-based compression or CNN/UNet processing (as in GEM), enabling low-dimensional bases and lightning-fast inference on commodity hardware.</p>  </li>  <li>    <p><strong>Decoupling Geometry &amp; Appearance</strong>By separating UV displacement for shape from UV textures for color and dynamics (MeGA), pipelines simplify rendering and editing complexity.</p>  </li>  <li>    <p><strong>Stable Barycentric Mapping</strong>Sampling in UV then projecting via barycentric coordinates (FATE) avoids mesh discretization artifacts and ensures smooth, differentiable optimization of Gaussian parameters.</p>  </li></ol><h3 id=\"conclusion\">Conclusion</h3><p>In sum, UV parameterizations transform complex 3D Gaussian splat operations into well-behaved 2D workflows, yielding more uniform sampling, intuitive editing, and powerful opportunities for compression and acceleration that mesh-only approaches simply cannot match.</p><h3 id=\"references\">References</h3><p>[1] FlashAvatar: Xiang, Jun, et al. “Flashavatar: High-fidelity head avatar with efficient gaussian embedding.” CVPR 2024.</p><p>[3] FATE: Zhang, Jiawei, et al. “Fate: Full-head gaussian avatar with textural editing from monocular video.” CVPR 2025.</p><p>[4] GEM: Zielonka, Wojciech, et al. “Gaussian eigen models for human heads.” CVPR 2025.</p><p>[2] MEGA: Wang, Cong, et al. “Mega: Hybrid mesh-gaussian head avatar for high-fidelity rendering and head editing.” CVPR 2025.</p>",
            "url": "http://localhost:4000/3d-gaussian-splats-avatars-utilizing-uv-mapping",
            
            
            
            
            
            "date_published": "2025-07-06T00:00:00+03:00",
            "date_modified": "2025-07-06T00:00:00+03:00",
            
                "author":  {
                "name": "Hüseyin Temiz",
                "url": null,
                "avatar": null
                }
                
            
        },
    
        {
            "id": "http://localhost:4000/3d-meshes-and-uv-mapping",
            "title": "3d Meshes And Uv Mapping",
            "summary": null,
            "content_text": "Meshes and UV maps are core to 3D graphics—they define shape and texture placement. From gaming and film to AR/VR and avatars, understanding how they work together is key to creating realistic and efficient digital content. This blog covers the basics and shows how they’re used in practice.3D Mesh and FLAMEA 3D mesh is a collection of vertices, edges, and faces that define the surface of a 3D object—like a wireframe sculpture. It forms the structural backbone of digital models used in games, animation, simulations, and 3D avatars. Common file formats include .obj, .ply, and .glb. For example, a human face mesh may consist of thousands of vertices and triangular faces to capture fine geometric detail.The FLAME [1] model (Faces Learned with an Articulated Model and Expressions) is a widely used parametric 3D head mesh model designed for realistic facial animation. It represents the face with 5023 vertices and 9976 triangles, controlled by identity, expression, and pose parameters. FLAME uses linear blend skinning (LBS) and blendshapes for expressive deformation, making it compact and efficient for applications like 3D avatars, facial reenactment, and neural rendering from 2D images or videos.  Figure 1: FLAME head mesh with different poses.[2]  FLAME’s parametric design uses 300 shape parameters (from PCA of 3D face scans) for identity control, 100 expression parameters for dynamic facial animation, and 6 pose parameters (3 neck, 3 jaw) for head/jaw rotation. By adjusting or tuning these parameters, we effectively modify the positions of FLAME’s 5023 vertices, enabling the generation of diverse facial shapes, expressions, and poses. This compact 406-parameter representation enables diverse, realistic faces with computational efficiency for real-time applications, as demonstrated in Figure 2 showing various shape, pose, and expression variations.  Figure 2: FLAME variation.  What Is UV Mapping?UV mapping is a 2D representation of a 3D surface used for texture mapping. Think of it as creating a flat blueprint of a curved 3D object—like unfolding a globe into a world map or unwrapping a chocolate bar to see its flat wrapper layout.Understanding U and V CoordinatesThe letters U and V represent 2D coordinates in texture space, similar to how X and Y work in regular 2D graphics:  U corresponds to the horizontal axis (0 to 1, left to right)  V corresponds to the vertical axis (0 to 1, bottom to top)  These coordinates are completely independent from the 3D XYZ coordinates of the meshThe Chocolate Bar AnalogyImagine unwrapping a chocolate bar:  3D Object: The wrapped chocolate bar has a complex curved surface  UV Map: When you carefully unfold the wrapper, you get a flat 2D layout  Texture Mapping: The printed design on the wrapper (logos, text, patterns) corresponds to how textures are applied to the 3D surfaceThis unwrapping process is exactly what UV mapping does—it takes the complex 3D surface and “unfolds” it into a flat 2D space where textures can be painted or applied.Why UV Coordinates?UV coordinates are normalized (0 to 1 range), making them resolution-independent. Whether your texture is 256×256 pixels or 4096×4096 pixels, the same UV coordinates will work perfectly.  Figure 3: UV MAP of FLAME.  How FLAME Mesh and UV Map Work TogetherThe FLAME model provides a 3D mesh of the human head, defined by 5023 vertices and 9976 triangular faces, while the pre-calculated UV map offers a 2D representation of that mesh surface. The matching between the FLAME mesh and its UV map is defined by a per-vertex correspondence, where each vertex in 3D space has an associated (u, v) coordinate in 2D UV space.Here’s how the matching works:  Each triangle in the FLAME mesh consists of 3 vertices indexed in 3D space.  The same triangle has corresponding UV coordinates for those 3 vertices in 2D.  When rendering or texturing, the 3D triangle is mapped to its 2D counterpart using these UV coordinates.  This mapping allows a 2D texture image to wrap seamlessly over the 3D mesh surface.In practice, you often have:  A vertex list: 3D coordinates (x, y, z) for each vertex.  A face list: triplets of vertex indices (defining triangles).  A UV list: (u, v) coordinates per vertex.  A UV face list: triplets of UV indices (corresponding to vertex indices in face list).So, the structure allows the renderer or processing tool to:  Match each triangle’s 3D geometry to its corresponding UV triangle.  Sample texture values from the UV space during rendering or neural feature mapping (e.g., in 3DGS or GEM).Techniques for UV MappingUV mapping can be done manually using tools like Blender or Maya for precise control, or automatically for faster results. Good UV maps minimize distortion and avoid stretching by carefully placing seams and organizing UV islands. Clean mapping is key for accurate texture placement and visual quality.Why UV Mapping MattersUV mapping is essential for applying textures, PBR materials, baked normals, and lightmaps, enabling detailed and stylized 3D visuals. It also plays a key role in neural rendering, such as mapping 3D Gaussians onto UV space for efficient and high-fidelity avatar synthesis.Tools and Code SnippetsTools like Trimesh and Open3D allow easy access to mesh geometry and UV coordinates in code. You can use them to load meshes, extract UVs, and sample points on the UV map for tasks like texture transfer or feature alignment. Advanced techniques like Poisson disk sampling on the UV map enable uniform point distribution, useful for generative models and neural rendering pipelines.ConclusionMesh and UV mapping form the foundation of 3D graphics, enabling detailed geometry and rich texture placement across digital models. Whether you’re working on games, films, or avatars, mastering these concepts is essential. Try loading a simple mesh and unwrapping it yourself to see how UVs bring textures to life—tools like Blender or code notebooks make it easy to get started.References[1] Li, Tianye, Timo Bolkart, Michael J. Black, Hao Li, and Javier Romero. “Learning a model of facial shape and expression from 4D scans.” ACM Trans. Graph. 2017.[2] https://github.com/TimoBolkart/FLAME-Universe",
            "content_html": "<p>Meshes and UV maps are core to 3D graphics—they define shape and texture placement. From gaming and film to AR/VR and avatars, understanding how they work together is key to creating realistic and efficient digital content. This blog covers the basics and shows how they’re used in practice.</p><h3 id=\"3d-mesh-and-flame\">3D Mesh and FLAME</h3><p>A 3D mesh is a collection of vertices, edges, and faces that define the surface of a 3D object—like a wireframe sculpture. It forms the structural backbone of digital models used in games, animation, simulations, and 3D avatars. Common file formats include .obj, .ply, and .glb. For example, a human face mesh may consist of thousands of vertices and triangular faces to capture fine geometric detail.</p><p>The FLAME [1] model (Faces Learned with an Articulated Model and Expressions) is a widely used parametric 3D head mesh model designed for realistic facial animation. It represents the face with 5023 vertices and 9976 triangles, controlled by identity, expression, and pose parameters. FLAME uses linear blend skinning (LBS) and blendshapes for expressive deformation, making it compact and efficient for applications like 3D avatars, facial reenactment, and neural rendering from 2D images or videos.</p><div align=\"center\">  <img src=\"assets/images/post002_flame_head.png\" alt=\"Nearest Neighbor Distance Comparison\" width=\"50%\" /><p><em>Figure 1: FLAME head mesh with different poses.[2]</em></p>  </div><!-- FLAME's parametric design uses **300 shape parameters** (from PCA of 3D face scans) for identity control, **100 expression parameters** for dynamic facial animation, and **6 pose parameters** (3 neck, 3 jaw) for head/jaw rotation. This compact 406-parameter representation enables diverse, realistic faces with computational efficiency for real-time applications, as demonstrated in Figure 2 showing various shape, pose, and expression variations.  --><p>FLAME’s parametric design uses <strong>300 shape parameters</strong> (from PCA of 3D face scans) for identity control, <strong>100 expression parameters</strong> for dynamic facial animation, and <strong>6 pose parameters</strong> (3 neck, 3 jaw) for head/jaw rotation. By adjusting or tuning these parameters, we effectively modify the positions of FLAME’s 5023 vertices, enabling the generation of diverse facial shapes, expressions, and poses. This compact 406-parameter representation enables diverse, realistic faces with computational efficiency for real-time applications, as demonstrated in Figure 2 showing various shape, pose, and expression variations.</p><div align=\"center\">  <img src=\"assets/images/post002_flame_model_variations.gif\" alt=\"Nearest Neighbor Distance Comparison\" width=\"80%\" /><p><em>Figure 2: FLAME variation.</em></p>  </div><h3 id=\"what-is-uv-mapping\">What Is UV Mapping?</h3><p><strong>UV mapping</strong> is a 2D representation of a 3D surface used for texture mapping. Think of it as creating a flat blueprint of a curved 3D object—like unfolding a globe into a world map or unwrapping a chocolate bar to see its flat wrapper layout.</p><h4 id=\"understanding-u-and-v-coordinates\">Understanding U and V Coordinates</h4><p>The letters <strong>U</strong> and <strong>V</strong> represent 2D coordinates in texture space, similar to how <strong>X</strong> and <strong>Y</strong> work in regular 2D graphics:</p><ul>  <li><strong>U</strong> corresponds to the horizontal axis (0 to 1, left to right)</li>  <li><strong>V</strong> corresponds to the vertical axis (0 to 1, bottom to top)</li>  <li>These coordinates are completely independent from the 3D <strong>XYZ</strong> coordinates of the mesh</li></ul><h4 id=\"the-chocolate-bar-analogy\">The Chocolate Bar Analogy</h4><p>Imagine unwrapping a chocolate bar:</p><ol>  <li><strong>3D Object</strong>: The wrapped chocolate bar has a complex curved surface</li>  <li><strong>UV Map</strong>: When you carefully unfold the wrapper, you get a flat 2D layout</li>  <li><strong>Texture Mapping</strong>: The printed design on the wrapper (logos, text, patterns) corresponds to how textures are applied to the 3D surface</li></ol><p>This unwrapping process is exactly what UV mapping does—it takes the complex 3D surface and “unfolds” it into a flat 2D space where textures can be painted or applied.</p><h4 id=\"why-uv-coordinates\">Why UV Coordinates?</h4><p>UV coordinates are normalized (0 to 1 range), making them resolution-independent. Whether your texture is 256×256 pixels or 4096×4096 pixels, the same UV coordinates will work perfectly.</p><div align=\"center\">  <img src=\"assets/images/post002_flame_uv_wireframe.png\" alt=\"Nearest Neighbor Distance Comparison\" width=\"55%\" /><p><em>Figure 3: UV MAP of FLAME.</em></p>  </div><h3 id=\"how-flame-mesh-and-uv-map-work-together\">How FLAME Mesh and UV Map Work Together</h3><p>The FLAME model provides a 3D mesh of the human head, defined by <strong>5023 vertices</strong> and <strong>9976 triangular faces</strong>, while the pre-calculated UV map offers a 2D representation of that mesh surface. The <strong>matching</strong> between the FLAME mesh and its UV map is defined by a <strong>per-vertex correspondence</strong>, where each vertex in 3D space has an associated <strong>(u, v)</strong> coordinate in 2D UV space.</p><p>Here’s how the matching works:</p><ul>  <li>Each triangle in the FLAME mesh consists of 3 vertices indexed in 3D space.</li>  <li>The same triangle has corresponding UV coordinates for those 3 vertices in 2D.</li>  <li>When rendering or texturing, the <strong>3D triangle</strong> is mapped to its <strong>2D counterpart</strong> using these UV coordinates.</li>  <li>This mapping allows a 2D texture image to wrap seamlessly over the 3D mesh surface.</li></ul><p>In practice, you often have:</p><ul>  <li>A <strong>vertex list</strong>: 3D coordinates (x, y, z) for each vertex.</li>  <li>A <strong>face list</strong>: triplets of vertex indices (defining triangles).</li>  <li>A <strong>UV list</strong>: (u, v) coordinates per vertex.</li>  <li>A <strong>UV face list</strong>: triplets of UV indices (corresponding to vertex indices in face list).</li></ul><p>So, the structure allows the renderer or processing tool to:</p><ul>  <li>Match each triangle’s 3D geometry to its corresponding UV triangle.</li>  <li>Sample texture values from the UV space during rendering or neural feature mapping (e.g., in 3DGS or GEM).</li></ul><h3 id=\"techniques-for-uv-mapping\">Techniques for UV Mapping</h3><p>UV mapping can be done manually using tools like Blender or Maya for precise control, or automatically for faster results. Good UV maps minimize distortion and avoid stretching by carefully placing seams and organizing UV islands. Clean mapping is key for accurate texture placement and visual quality.</p><h3 id=\"why-uv-mapping-matters\">Why UV Mapping Matters</h3><p>UV mapping is essential for applying textures, PBR materials, baked normals, and lightmaps, enabling detailed and stylized 3D visuals. It also plays a key role in neural rendering, such as mapping 3D Gaussians onto UV space for efficient and high-fidelity avatar synthesis.</p><h3 id=\"tools-and-code-snippets\">Tools and Code Snippets</h3><p>Tools like Trimesh and Open3D allow easy access to mesh geometry and UV coordinates in code. You can use them to load meshes, extract UVs, and sample points on the UV map for tasks like texture transfer or feature alignment. Advanced techniques like Poisson disk sampling on the UV map enable uniform point distribution, useful for generative models and neural rendering pipelines.</p><h3 id=\"conclusion\">Conclusion</h3><p>Mesh and UV mapping form the foundation of 3D graphics, enabling detailed geometry and rich texture placement across digital models. Whether you’re working on games, films, or avatars, mastering these concepts is essential. Try loading a simple mesh and unwrapping it yourself to see how UVs bring textures to life—tools like Blender or code notebooks make it easy to get started.</p><h3 id=\"references\">References</h3><p>[1] Li, Tianye, Timo Bolkart, Michael J. Black, Hao Li, and Javier Romero. “Learning a model of facial shape and expression from 4D scans.” ACM Trans. Graph. 2017.</p><p>[2] https://github.com/TimoBolkart/FLAME-Universe</p>",
            "url": "http://localhost:4000/3d-meshes-and-uv-mapping",
            
            
            
            
            
            "date_published": "2025-07-04T00:00:00+03:00",
            "date_modified": "2025-07-04T00:00:00+03:00",
            
                "author":  {
                "name": "Hüseyin Temiz",
                "url": null,
                "avatar": null
                }
                
            
        },
    
        {
            "id": "http://localhost:4000/placing-gaussians-splats-on-mesh-surface",
            "title": "Placing Gaussians Splats On Mesh Surface",
            "summary": null,
            "content_text": "In recent years, 3D Gaussian Splatting (GS) has emerged as a powerful technique for photorealistic avatar rendering and animation. Optimizing Gaussian splats without any geometric prior is challenging, as they often fail to regress accurate geometry. However, when the geometry of the object is known, fitting a mesh in advance and using its surface as a geometric prior provides a significant advantage. For example, if the object to be reconstructed is a human head, leveraging the FLAME mesh model as a geometric prior can greatly improve results.After fitting the FLAME model, we obtain a reasonable geometric surface on which to place GSs. At this stage, how we distribute the GSs on the mesh becomes an important problem. Using a smart strategy to place the GSs on the surface can significantly aid the subsequent GS optimization process.Naïve Random SamplingAs a baseline, GSs can be randomly distributed across the mesh surface. However, this naive approach often leads to clustering, irregular spacing, and oversampling, which degrades both visual quality and optimization stability.Poisson Disk SamplingTo avoid the performance degradations of random sampling, Poisson Disk Sampling — a technique that enforces a minimum distance between sampled points — to robustly distribute Gaussians on the FLAME mesh.Poisson Disk Sampling Algorithm (Bridson’s Algorithm):  Initialize a grid with cell size = r/√2 (where r is minimum distance)  Start with a random initial point, add to active list  While active list is not empty:          Pick random point from active list      Generate k candidates (typically k=30) in annulus [r, 2r]      For each candidate:              Check if it’s far enough from all neighbors      If valid, add to output and active list      - If no valid candidates found, remove point from active list      Key Properties:  Guarantees minimum distance r between all points  Maximum distance approximately 2r  O(n) time complexity  Produces uniform, organic-looking distributions     Figure 1: A side-by-side comparison of random sampling (left) and Poisson disk sampling (right) on a 2D surface.Figure 1 shows the comparison between the two sampling methods, highlighting how Poisson Disk Sampling achieves more consistent nearest neighbor distances (lower standard deviation) and avoids extreme clustering (higher minimum distance) compared to random sampling.      Figure 2: Comparison between random sampling (left) and Poisson disk sampling (right) on a sphere surface.As Figure 2 shows, Poisson disk sampling provides significantly better surface coverage compared to random sampling. The uniform distribution of sample points ensures that no regions of the mesh are left sparsely covered, while simultaneously preventing overcrowding in other areas. This improved coverage translates directly to more consistent Gaussian placement across the entire FLAME mesh surface, resulting in better geometric representation and more stable optimization during the 3D Gaussian splatting process.  Figure 3: Nearest neighbor distance distributions for different samplings on sphere surface.              Metric      Poisson Disk Sampling      Random Sampling                  n_points      500      500              nn_mean      0.1187      0.0787              nn_std      0.0160      0.0418              nn_min      0.1001      0.0073              nn_max      0.1853      0.2509      Table 1: Nearest neighbor distance statistics comparison between Poisson Disk Sampling and Random Sampling methods.The quantitative analysis in Table 1 and the distribution visualization in Figure 3 clearly demonstrate the superiority of Poisson Disk Sampling over random sampling. Most notably, Poisson Disk Sampling maintains a significantly higher minimum distance (0.1001 vs 0.0073), preventing the extreme clustering that occurs with random sampling. The lower standard deviation (0.0160 vs 0.0418) indicates much more consistent spacing between neighboring Gaussians, while the constrained maximum distance (0.1853 vs 0.2509) ensures no large gaps in coverage. This uniform distribution is crucial for stable 3D Gaussian Splatting optimization, as it prevents both overcrowded regions that cause rendering artifacts and sparse areas that miss geometric detail.Sampling on FLAME meshFigures 2 and 3 illustrate performance on a spherical surface with a more uniform geometry. In real-world applications, geometry is far from trivial—the FLAME human head mesh is both highly realistic and sufficiently complex. We will evaluate the sampling methods’ performance on the FLAME model.  Figure 4: Nearest neighbor distance distributions for different samplings.  Figure 4 visually compares random and Poisson disk sampling on the FLAME mesh for different point counts. Poisson disk sampling (bottom row, red) produces a much more uniform and evenly spaced distribution of points across the surface, as reflected by higher uniformity scores. In contrast, random sampling (top row, blue) results in visible clustering and uneven coverage, regardless of the number of points. This demonstrates that Poisson disk sampling maintains consistent regularity and superior surface coverage.  Figure 5: Nearest Neihbor distance distribution of different sampling experiments.  In Figure 5, Poisson disk sampling consistently enforces a minimum distance between neighboring samples, as shown by the sharp cutoff on the left side of each distribution. This property prevents samples from clustering too closely or overlapping, resulting in a more uniform and controlled distribution across the surface. In contrast, random sampling produces a broader spread of nearest neighbor distances, with many samples packed very closely together, leading to potential overlaps and uneven coverage. As the number of points increases, Poisson sampling maintains this regularity, while random sampling continues to exhibit significant variability and clustering. This highlights the robustness of Poisson disk sampling for applications requiring uniform coverage.  Figure 6: flame_sampling_statistics.png.  The Uniformity Score is a metric that quantifies how evenly points are distributed in a sampling. It is calculated as 1 minus the coefficient of variation (CV) of the nearest neighbor distances, where CV is the standard deviation divided by the mean. A score closer to 1 indicates a more uniform (evenly spaced) distribution, while a lower score means greater variability and less uniformity. This makes it easy to compare different sampling methods: for example, Poisson disk sampling yields higher uniformity scores than random sampling, reflecting its more consistent spacing between points.            Points      Random Sampling      Poisson Disk Sampling                  1000      0.4906      0.7752              5000      0.4760      0.8107              10000      0.4795      0.8086      Table 2: Uniformity scores (higher is better) for random and Poisson disk sampling at different point counts.Table 2 clearly shows that Poisson disk sampling consistently achieves much higher uniformity scores than random sampling across all point counts. This indicates that Poisson disk sampling produces a more evenly spaced and regular distribution of points on the mesh surface, while random sampling results in greater variability and less uniform coverage. The higher uniformity of Poisson disk sampling is especially important for applications that require consistent surface representation and stable optimization.ConclusionFor placing Gaussians on mesh surfaces, uniform coverage, minimal overlap, adaptive density, and mesh-awareness are essential. Naïve random sampling often leads to clustering, gaps, and unstable optimization. In contrast, Poisson Disk Sampling enforces even spacing and respects mesh geometry, resulting in a more regular and robust distribution. Having better geometric priors at the start of Gaussian splat optimization leads to more accurate and consistent recovery of visual details. This approach enables more accurate and stable neural avatar representations, improving rendering quality and optimization convergence—especially for complex models and advanced techniques such as GEM, RGBAvatar, or 3D Gaussian Blendshapes.",
            "content_html": "<!-- ## Placing Gaussians on Mesh Surface --><p>In recent years, 3D Gaussian Splatting (GS) has emerged as a powerful technique for photorealistic avatar rendering and animation. Optimizing Gaussian splats without any geometric prior is challenging, as they often fail to regress accurate geometry. However, when the geometry of the object is known, fitting a mesh in advance and using its surface as a geometric prior provides a significant advantage. For example, if the object to be reconstructed is a human head, leveraging the FLAME mesh model as a geometric prior can greatly improve results.</p><p>After fitting the FLAME model, we obtain a reasonable geometric surface on which to place GSs. At this stage, how we distribute the GSs on the mesh becomes an important problem. Using a smart strategy to place the GSs on the surface can significantly aid the subsequent GS optimization process.</p><h3 id=\"naïve-random-sampling\">Naïve Random Sampling</h3><p>As a baseline, GSs can be randomly distributed across the mesh surface. However, this naive approach often leads to <strong>clustering</strong>, <strong>irregular spacing</strong>, and <strong>oversampling</strong>, which degrades both visual quality and optimization stability.</p><h3 id=\"poisson-disk-sampling\">Poisson Disk Sampling</h3><p>To avoid the performance degradations of random sampling, <strong>Poisson Disk Sampling</strong> — a technique that enforces a minimum distance between sampled points — to robustly distribute Gaussians on the FLAME mesh.</p><p><strong>Poisson Disk Sampling Algorithm (Bridson’s Algorithm)</strong>:</p><ol>  <li>Initialize a grid with cell size = r/√2 (where r is minimum distance)</li>  <li>Start with a random initial point, add to active list</li>  <li>While active list is not empty:    <ul>      <li>Pick random point from active list</li>      <li>Generate k candidates (typically k=30) in annulus [r, 2r]</li>      <li>For each candidate:</li>    </ul>    <ul>      <li>Check if it’s far enough from all neighbors</li>      <li>If valid, add to output and active list      - If no valid candidates found, remove point from active list</li>    </ul>  </li></ol><p><strong>Key Properties</strong>:</p><ul>  <li>Guarantees minimum distance r between all points</li>  <li>Maximum distance approximately 2r</li>  <li>O(n) time complexity</li>  <li>Produces uniform, organic-looking distributions</li></ul><div align=\"center\">  <img src=\"assets/images/post001_algorithm_comparison.png\" alt=\"Algorithm Comparison\" width=\"70%\" />  <p><em> Figure 1: A side-by-side comparison of random sampling (left) and Poisson disk sampling (right) on a 2D surface.</em></p></div><!-- /Users/huseyintemiz/Documents/temiz_ai_dev/poisson_disk/experiments1/run_visualization.py --><p>Figure 1 shows the comparison between the two sampling methods, highlighting how Poisson Disk Sampling achieves more consistent nearest neighbor distances (lower standard deviation) and avoids extreme clustering (higher minimum distance) compared to random sampling.</p><div align=\"center\">  <img src=\"assets/images/post001_sampling_comparison.png\" alt=\"Sampling Comparison on Surface\" width=\"70%\" />    <p><em>Figure 2: Comparison between random sampling (left) and Poisson disk sampling (right) on a sphere surface.</em></p></div><p>As Figure 2 shows, Poisson disk sampling provides significantly better surface coverage compared to random sampling. The uniform distribution of sample points ensures that no regions of the mesh are left sparsely covered, while simultaneously preventing overcrowding in other areas. This improved coverage translates directly to more consistent Gaussian placement across the entire FLAME mesh surface, resulting in better geometric representation and more stable optimization during the 3D Gaussian splatting process.</p><div align=\"center\">  <img src=\"assets/images/post001_nn_distance_comparison.png\" alt=\"Nearest Neighbor Distance Comparison\" width=\"50%\" /><p><em>Figure 3: Nearest neighbor distance distributions for different samplings on sphere surface.</em></p>  </div><table>  <thead>    <tr>      <th>Metric</th>      <th>Poisson Disk Sampling</th>      <th>Random Sampling</th>    </tr>  </thead>  <tbody>    <tr>      <td><strong>n_points</strong></td>      <td>500</td>      <td>500</td>    </tr>    <tr>      <td><strong>nn_mean</strong></td>      <td>0.1187</td>      <td>0.0787</td>    </tr>    <tr>      <td><strong>nn_std</strong></td>      <td>0.0160</td>      <td>0.0418</td>    </tr>    <tr>      <td><strong>nn_min</strong></td>      <td>0.1001</td>      <td>0.0073</td>    </tr>    <tr>      <td><strong>nn_max</strong></td>      <td>0.1853</td>      <td>0.2509</td>    </tr>  </tbody></table><p><em>Table 1: Nearest neighbor distance statistics comparison between Poisson Disk Sampling and Random Sampling methods.</em></p><p>The quantitative analysis in Table 1 and the distribution visualization in Figure 3 clearly demonstrate the superiority of Poisson Disk Sampling over random sampling. Most notably, Poisson Disk Sampling maintains a <strong>significantly higher minimum distance</strong> (0.1001 vs 0.0073), preventing the extreme clustering that occurs with random sampling. The <strong>lower standard deviation</strong> (0.0160 vs 0.0418) indicates much more consistent spacing between neighboring Gaussians, while the <strong>constrained maximum distance</strong> (0.1853 vs 0.2509) ensures no large gaps in coverage. This uniform distribution is crucial for stable 3D Gaussian Splatting optimization, as it prevents both overcrowded regions that cause rendering artifacts and sparse areas that miss geometric detail.</p><h3 id=\"sampling-on-flame-mesh\">Sampling on FLAME mesh</h3><p>Figures 2 and 3 illustrate performance on a spherical surface with a more uniform geometry. In real-world applications, geometry is far from trivial—the FLAME human head mesh is both highly realistic and sufficiently complex. We will evaluate the sampling methods’ performance on the FLAME model.</p><div align=\"center\">  <img src=\"assets/images/post001_flame_sampling_comparison.png\" alt=\"Nearest Neighbor Distance Comparison\" width=\"50%\" /><p><em>Figure 4: Nearest neighbor distance distributions for different samplings.</em></p>  </div><p>Figure 4 visually compares random and Poisson disk sampling on the FLAME mesh for different point counts. Poisson disk sampling (bottom row, red) produces a much more uniform and evenly spaced distribution of points across the surface, as reflected by higher uniformity scores. In contrast, random sampling (top row, blue) results in visible clustering and uneven coverage, regardless of the number of points. This demonstrates that Poisson disk sampling maintains consistent regularity and superior surface coverage.</p><div align=\"center\">  <img src=\"assets/images/post001_flame_nn_distance_histograms.png\" alt=\"Nearest Neighbor Distance Comparison\" width=\"50%\" /><p><em>Figure 5: Nearest Neihbor distance distribution of different sampling experiments.</em></p>  </div><p>In Figure 5, Poisson disk sampling consistently enforces a minimum distance between neighboring samples, as shown by the sharp cutoff on the left side of each distribution. This property prevents samples from clustering too closely or overlapping, resulting in a more uniform and controlled distribution across the surface. In contrast, random sampling produces a broader spread of nearest neighbor distances, with many samples packed very closely together, leading to potential overlaps and uneven coverage. As the number of points increases, Poisson sampling maintains this regularity, while random sampling continues to exhibit significant variability and clustering. This highlights the robustness of Poisson disk sampling for applications requiring uniform coverage.</p><div align=\"center\">  <img src=\"assets/images/post001_flame_sampling_statistics.png\" alt=\"Nearest Neighbor Distance Comparison\" width=\"50%\" /><p><em>Figure 6: flame_sampling_statistics.png.</em></p>  </div><p>The Uniformity Score is a metric that quantifies how evenly points are distributed in a sampling. It is calculated as 1 minus the coefficient of variation (CV) of the nearest neighbor distances, where CV is the standard deviation divided by the mean. A score closer to 1 indicates a more uniform (evenly spaced) distribution, while a lower score means greater variability and less uniformity. This makes it easy to compare different sampling methods: for example, Poisson disk sampling yields higher uniformity scores than random sampling, reflecting its more consistent spacing between points.</p><table>  <thead>    <tr>      <th>Points</th>      <th>Random Sampling</th>      <th>Poisson Disk Sampling</th>    </tr>  </thead>  <tbody>    <tr>      <td>1000</td>      <td>0.4906</td>      <td>0.7752</td>    </tr>    <tr>      <td>5000</td>      <td>0.4760</td>      <td>0.8107</td>    </tr>    <tr>      <td>10000</td>      <td>0.4795</td>      <td>0.8086</td>    </tr>  </tbody></table><p><em>Table 2: Uniformity scores (higher is better) for random and Poisson disk sampling at different point counts.</em></p><p>Table 2 clearly shows that Poisson disk sampling consistently achieves much higher uniformity scores than random sampling across all point counts. This indicates that Poisson disk sampling produces a more evenly spaced and regular distribution of points on the mesh surface, while random sampling results in greater variability and less uniform coverage. The higher uniformity of Poisson disk sampling is especially important for applications that require consistent surface representation and stable optimization.</p><h2 id=\"conclusion\">Conclusion</h2><p>For placing Gaussians on mesh surfaces, uniform coverage, minimal overlap, adaptive density, and mesh-awareness are essential. Naïve random sampling often leads to clustering, gaps, and unstable optimization. In contrast, Poisson Disk Sampling enforces even spacing and respects mesh geometry, resulting in a more regular and robust distribution. Having better geometric priors at the start of Gaussian splat optimization leads to more accurate and consistent recovery of visual details. This approach enables more accurate and stable neural avatar representations, improving rendering quality and optimization convergence—especially for complex models and advanced techniques such as GEM, RGBAvatar, or 3D Gaussian Blendshapes.</p><!-- #### to-do list- poisson disk sampling logic/algorithm- complexity check- PD init GS vs Random init GS (compare metrics) -->",
            "url": "http://localhost:4000/placing-gaussians-splats-on-mesh-surface",
            
            
            
            
            
            "date_published": "2025-07-03T00:00:00+03:00",
            "date_modified": "2025-07-03T00:00:00+03:00",
            
                "author":  {
                "name": "Hüseyin Temiz",
                "url": null,
                "avatar": null
                }
                
            
        },
    
        {
            "id": "http://localhost:4000/sample-math-latex-code",
            "title": "Sample Math Latex Code",
            "summary": null,
            "content_text": "This is experimental page. All contents are garbage.    Set of spherical harmonics mapped to the surface of a sphereWhen rendering 3D scenes using Gaussian splats, each point (or “splat”) needs to represent not just color, but how that color changes depending on the viewing direction. Spherical harmonics allow encoding this angular variation compactly.This is important for achieving photorealistic rendering with effects like:  Specular highlights  Soft shadows  View-dependent lighting changes🔸 What Are Spherical Harmonics?Spherical Harmonics (SH) are a series of orthogonal basis functions defined on the surface of a sphere. You can think of them as the 3D analog of Fourier series for functions on a sphere.They are indexed by:  Degree (l): controls the frequency (higher means more detail)  Order (m): varies from -l to +lExample Levels:  SH level 0 (SH0): Only constant color (no view dependence)  SH level 1 (SH1): Basic directional lighting (like Lambertian)  SH level 2 or 3+: Captures more complex angular dependencies🔸 SH in Gaussian SplattingEach 3D Gaussian stores:  Position, scale, rotation, opacity  Spherical Harmonics coefficients for RGB appearanceInstead of storing just a single RGB color, we store a set of SH coefficients per color channel. When rendering:  Compute the viewing direction v for each Gaussian.  Evaluate the SH basis functions at direction v to get a vector SH_basis(v).  Compute the final color as a dot product:\\[C(v) = \\sum_{l=0}^{L} \\sum_{m=-l}^{l} c_{l,m} \\cdot Y_{l,m}(v)\\]Where:  \\(C(v)\\) : The final color for the viewing direction \\(v\\).  \\(c_{l,m}\\): The SH coefficients for each degree \\(l\\) and order \\(m\\).  \\(Y_{l,m}(v)\\): The SH basis functions evaluated at the direction \\(v\\).🔸 Key Spherical Harmonics EquationsSH Basis FunctionsThe real-valued spherical harmonics basis functions are defined as:\\[Y_{l}^{m}(\\theta, \\phi) = N_{l}^{m} \\cdot P_{l}^{m}(\\cos\\theta) \\cdot \\begin{cases}\\sqrt{2} \\sin(|m|\\phi) &amp; \\text{if } m &lt; 0 \\\\1 &amp; \\text{if } m = 0 \\\\\\sqrt{2} \\cos(m\\phi) &amp; \\text{if } m &gt; 0\\end{cases}\\]Where:  \\(\\theta\\) : inclination (angle from the z-axis)  \\(\\phi\\): azimuth (angle from the x-axis in the x-y plane)  \\(N_{l}^{m}\\): normalization constant  \\(P_{l}^{m}\\): associated Legendre polynomialNormalization Constant\\[N_{l}^{m} = \\sqrt{\\frac{(2l+1)}{4\\pi} \\cdot \\frac{(l - |m|)!}{(l + |m|)!}}\\]Example: SH0 and SH1      SH0 (l=0, m=0):  \\(Y_0^0 = \\frac{1}{2} \\sqrt{\\frac{1}{\\pi}}\\)    SH1 (l=1, m=-1,0,1):  \\(Y_1^{-1} = \\sqrt{\\frac{3}{4\\pi}} \\sin\\theta \\sin\\phi\\)  \\(Y_1^{0} = \\sqrt{\\frac{3}{4\\pi}} \\cos\\theta\\)  \\(Y_1^{1} = \\sqrt{\\frac{3}{4\\pi}} \\sin\\theta \\cos\\phi\\)    🔸 Benefits of Using SH    Compact representation of angular variation.  Efficient evaluation during rendering.  Scalable to higher levels for more detail.",
            "content_html": "<!-- # Spherical Harmonics in Gaussian Splats --><p>This is experimental page. All contents are garbage.</p><!-- ## 🔸 Why Use Spherical Harmonics in Gaussian Splats? --><p align=\"center\">    <img src=\"assets/images/post000_set-of-spherical-harmonics-mapped-to-the-surface-of-a-sphere.png\" alt=\"Set of spherical harmonics mapped to the surface of a sphere\" width=\"400\" /></p><p align=\"center\"><em>Set of spherical harmonics mapped to the surface of a sphere</em></p><p>When rendering 3D scenes using Gaussian splats, each point (or “splat”) needs to represent not just <strong>color</strong>, but how that color <strong>changes depending on the viewing direction</strong>. Spherical harmonics allow encoding this angular variation compactly.</p><p>This is important for achieving <strong>photorealistic rendering</strong> with effects like:</p><ul>  <li>Specular highlights</li>  <li>Soft shadows</li>  <li>View-dependent lighting changes</li></ul><h2 id=\"-what-are-spherical-harmonics\">🔸 What Are Spherical Harmonics?</h2><p><strong>Spherical Harmonics (SH)</strong> are a series of <strong>orthogonal basis functions</strong> defined on the surface of a sphere. You can think of them as the 3D analog of <strong>Fourier series</strong> for functions on a sphere.</p><p>They are indexed by:</p><ul>  <li><strong>Degree (l)</strong>: controls the frequency (higher means more detail)</li>  <li><strong>Order (m)</strong>: varies from <code class=\"language-plaintext highlighter-rouge\">-l</code> to <code class=\"language-plaintext highlighter-rouge\">+l</code></li></ul><h3 id=\"example-levels\">Example Levels:</h3><ul>  <li><strong>SH level 0 (SH0):</strong> Only constant color (no view dependence)</li>  <li><strong>SH level 1 (SH1):</strong> Basic directional lighting (like Lambertian)</li>  <li><strong>SH level 2 or 3+:</strong> Captures more complex angular dependencies</li></ul><h2 id=\"-sh-in-gaussian-splatting\">🔸 SH in Gaussian Splatting</h2><p>Each 3D Gaussian stores:</p><ul>  <li>Position, scale, rotation, opacity</li>  <li><strong>Spherical Harmonics coefficients for RGB appearance</strong></li></ul><p>Instead of storing just a single RGB color, we store a <strong>set of SH coefficients per color channel</strong>. When rendering:</p><ol>  <li>Compute the <strong>viewing direction</strong> <code class=\"language-plaintext highlighter-rouge\">v</code> for each Gaussian.</li>  <li>Evaluate the <strong>SH basis functions</strong> at direction <code class=\"language-plaintext highlighter-rouge\">v</code> to get a vector <code class=\"language-plaintext highlighter-rouge\">SH_basis(v)</code>.</li>  <li>Compute the final color as a dot product:</li></ol>\\[C(v) = \\sum_{l=0}^{L} \\sum_{m=-l}^{l} c_{l,m} \\cdot Y_{l,m}(v)\\]<p>Where:</p><ul>  <li>\\(C(v)\\) : The final color for the viewing direction \\(v\\).</li>  <li>\\(c_{l,m}\\): The SH coefficients for each degree \\(l\\) and order \\(m\\).</li>  <li>\\(Y_{l,m}(v)\\): The SH basis functions evaluated at the direction \\(v\\).</li></ul><h2 id=\"-key-spherical-harmonics-equations\">🔸 Key Spherical Harmonics Equations</h2><h3 id=\"sh-basis-functions\">SH Basis Functions</h3><p>The real-valued spherical harmonics basis functions are defined as:</p>\\[Y_{l}^{m}(\\theta, \\phi) = N_{l}^{m} \\cdot P_{l}^{m}(\\cos\\theta) \\cdot \\begin{cases}\\sqrt{2} \\sin(|m|\\phi) &amp; \\text{if } m &lt; 0 \\\\1 &amp; \\text{if } m = 0 \\\\\\sqrt{2} \\cos(m\\phi) &amp; \\text{if } m &gt; 0\\end{cases}\\]<p>Where:</p><ul>  <li>\\(\\theta\\) : inclination (angle from the z-axis)</li>  <li>\\(\\phi\\): azimuth (angle from the x-axis in the x-y plane)</li>  <li>\\(N_{l}^{m}\\): normalization constant</li>  <li>\\(P_{l}^{m}\\): associated Legendre polynomial</li></ul><h3 id=\"normalization-constant\">Normalization Constant</h3>\\[N_{l}^{m} = \\sqrt{\\frac{(2l+1)}{4\\pi} \\cdot \\frac{(l - |m|)!}{(l + |m|)!}}\\]<h3 id=\"example-sh0-and-sh1\">Example: SH0 and SH1</h3><ul>  <li>    <p><strong>SH0 (l=0, m=0):</strong>  \\(Y_0^0 = \\frac{1}{2} \\sqrt{\\frac{1}{\\pi}}\\)</p>  </li>  <li><strong>SH1 (l=1, m=-1,0,1):</strong>  \\(Y_1^{-1} = \\sqrt{\\frac{3}{4\\pi}} \\sin\\theta \\sin\\phi\\)  \\(Y_1^{0} = \\sqrt{\\frac{3}{4\\pi}} \\cos\\theta\\)  \\(Y_1^{1} = \\sqrt{\\frac{3}{4\\pi}} \\sin\\theta \\cos\\phi\\)    <h2 id=\"-benefits-of-using-sh\">🔸 Benefits of Using SH</h2>  </li>  <li>Compact representation of angular variation.</li>  <li>Efficient evaluation during rendering.</li>  <li>Scalable to higher levels for more detail.</li></ul>",
            "url": "http://localhost:4000/sample-math-latex-code",
            
            
            
            
            
            "date_published": "2025-05-20T00:00:00+03:00",
            "date_modified": "2025-05-20T00:00:00+03:00",
            
                "author":  {
                "name": "Hüseyin Temiz",
                "url": null,
                "avatar": null
                }
                
            
        },
    
        {
            "id": "http://localhost:4000/spherical-harmonics-in-gaussian-splats",
            "title": "Spherical Harmonics In Gaussian Splats",
            "summary": null,
            "content_text": "🔸 Why Use Spherical Harmonics in Gaussian Splats?When rendering 3D scenes using Gaussian splats, each point (or “splat”) needs to represent not just color, but how that color changes depending on the viewing direction. Spherical harmonics allow encoding this angular variation compactly.            Left: Spherical harmonics mapped to a sphere &nbsp;&nbsp;|&nbsp;&nbsp; Right: Spherical harmonics visualizationThis is important for achieving photorealistic rendering with effects like:  Specular highlights  Soft shadows  View-dependent lighting changes🔸 What Are Spherical Harmonics?Spherical Harmonics (SH) are a series of orthogonal basis functions defined on the surface of a sphere. You can think of them as the 3D analog of Fourier series for functions on a sphere.They are indexed by:  Degree (l): controls the frequency (higher means more detail)  Order (m): varies from -l to +lExample Levels:  SH level 0 (SH0): Only constant color (no view dependence)  SH level 1 (SH1): Basic directional lighting (like Lambertian)  SH level 2 or 3+: Captures more complex angular dependencies🔸 SH in Gaussian SplattingEach 3D Gaussian stores:  Position, scale, rotation, opacity  Spherical Harmonics coefficients for RGB appearanceInstead of storing just a single RGB color, we store a set of SH coefficients per color channel. When rendering:  Compute the viewing direction v for each Gaussian.  Evaluate the SH basis functions at direction v to get a vector SH_basis(v).  Compute the final color as a dot product:color = Σ (c_i * SH_basis_i(v))   for i = 1..nwhere c_i are SH coefficients.🔸 SH Level Examples            SH Level      # Coefficients (per color)      What it Captures                  0      1      Constant color              1      4      Directional diffuse lighting              2      9      Soft shadows, simple specular              3      16      Rich angular detail      So if SH=2:  You store 9 SH coefficients per color channel (27 total for RGB).  You can model how color changes with view angle in a smooth and efficient way.🔸 Visual Example (Simplified)Imagine a glossy red ball:  Without SH (SH=0): The ball is flat red from all directions.  With SH=1: The ball appears brighter on the side facing the light source.  With SH=2+: The ball has soft reflections and looks more realistic as you move the camera.🔸 Dummy Python Code Exampleimport numpy as npfrom scipy.special import sph_harm# Dummy view direction (theta, phi) in spherical coordinatestheta = np.pi / 4  # angle from z-axisphi = np.pi / 3    # angle from x-axis in x-y plane# Compute SH basis up to degree l=2def compute_sh_basis(theta, phi, l_max=2):    basis = []    for l in range(l_max + 1):        for m in range(-l, l + 1):            Y_lm = sph_harm(m, l, phi, theta).real  # Only real part used in graphics            basis.append(Y_lm)    return np.array(basis)# Example SH coefficients for RGB (assuming SH2 = 9 coeffs per channel)sh_coeffs_r = np.random.rand(9)sh_coeffs_g = np.random.rand(9)sh_coeffs_b = np.random.rand(9)# Evaluate SH basissh_basis = compute_sh_basis(theta, phi)# Final RGB color (dot product of SH coefficients and SH basis)r = np.dot(sh_coeffs_r, sh_basis)g = np.dot(sh_coeffs_g, sh_basis)b = np.dot(sh_coeffs_b, sh_basis)print(f\"RGB color from SH: R={r:.2f}, G={g:.2f}, B={b:.2f}\")This code shows:  How SH basis functions are computed using scipy.special.sph_harm  How SH coefficients are applied to compute view-dependent color🔸 SummaryIn Gaussian Splatting, Spherical Harmonics:  Encode how color/appearance changes with view direction  Provide a compact way to achieve realistic rendering  Are used per-splat (each Gaussian has its own SH coefficients)You can trade off quality vs speed by choosing the SH level (e.g., SH=0 for fast rendering, SH=2+ for better realism).",
            "content_html": "<!-- # Spherical Harmonics in Gaussian Splats --><h2 id=\"-why-use-spherical-harmonics-in-gaussian-splats\">🔸 Why Use Spherical Harmonics in Gaussian Splats?</h2><p>When rendering 3D scenes using Gaussian splats, each point (or “splat”) needs to represent not just <strong>color</strong>, but how that color <strong>changes depending on the viewing direction</strong>. Spherical harmonics allow encoding this angular variation compactly.</p><p align=\"center\">    <img src=\"assets/images/post000_set-of-spherical-harmonics-mapped-to-the-surface-of-a-sphere.png\" alt=\"Set of spherical harmonics mapped to the surface of a sphere\" width=\"300\" />    <img src=\"assets/images/post000_Spherical_Harmonics.png\" alt=\"Spherical Harmonics visualization\" width=\"400\" /></p><p align=\"center\">    <em>Left: Spherical harmonics mapped to a sphere &nbsp;&nbsp;|&nbsp;&nbsp; Right: Spherical harmonics visualization</em></p><p>This is important for achieving <strong>photorealistic rendering</strong> with effects like:</p><ul>  <li>Specular highlights</li>  <li>Soft shadows</li>  <li>View-dependent lighting changes</li></ul><h2 id=\"-what-are-spherical-harmonics\">🔸 What Are Spherical Harmonics?</h2><p><strong>Spherical Harmonics (SH)</strong> are a series of <strong>orthogonal basis functions</strong> defined on the surface of a sphere. You can think of them as the 3D analog of <strong>Fourier series</strong> for functions on a sphere.</p><p>They are indexed by:</p><ul>  <li><strong>Degree (l)</strong>: controls the frequency (higher means more detail)</li>  <li><strong>Order (m)</strong>: varies from <code class=\"language-plaintext highlighter-rouge\">-l</code> to <code class=\"language-plaintext highlighter-rouge\">+l</code></li></ul><h3 id=\"example-levels\">Example Levels:</h3><ul>  <li><strong>SH level 0 (SH0):</strong> Only constant color (no view dependence)</li>  <li><strong>SH level 1 (SH1):</strong> Basic directional lighting (like Lambertian)</li>  <li><strong>SH level 2 or 3+:</strong> Captures more complex angular dependencies</li></ul><h2 id=\"-sh-in-gaussian-splatting\">🔸 SH in Gaussian Splatting</h2><p>Each 3D Gaussian stores:</p><ul>  <li>Position, scale, rotation, opacity</li>  <li><strong>Spherical Harmonics coefficients for RGB appearance</strong></li></ul><p>Instead of storing just a single RGB color, we store a <strong>set of SH coefficients per color channel</strong>. When rendering:</p><ol>  <li>Compute the <strong>viewing direction</strong> <code class=\"language-plaintext highlighter-rouge\">v</code> for each Gaussian.</li>  <li>Evaluate the <strong>SH basis functions</strong> at direction <code class=\"language-plaintext highlighter-rouge\">v</code> to get a vector <code class=\"language-plaintext highlighter-rouge\">SH_basis(v)</code>.</li>  <li>Compute the final color as a dot product:</li></ol><div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>color = Σ (c_i * SH_basis_i(v))   for i = 1..n</code></pre></div></div><p>where <code class=\"language-plaintext highlighter-rouge\">c_i</code> are SH coefficients.</p><h2 id=\"-sh-level-examples\">🔸 SH Level Examples</h2><table>  <thead>    <tr>      <th>SH Level</th>      <th># Coefficients (per color)</th>      <th>What it Captures</th>    </tr>  </thead>  <tbody>    <tr>      <td>0</td>      <td>1</td>      <td>Constant color</td>    </tr>    <tr>      <td>1</td>      <td>4</td>      <td>Directional diffuse lighting</td>    </tr>    <tr>      <td>2</td>      <td>9</td>      <td>Soft shadows, simple specular</td>    </tr>    <tr>      <td>3</td>      <td>16</td>      <td>Rich angular detail</td>    </tr>  </tbody></table><p>So if SH=2:</p><ul>  <li>You store 9 SH coefficients per color channel (27 total for RGB).</li>  <li>You can model how color changes with view angle in a smooth and efficient way.</li></ul><h2 id=\"-visual-example-simplified\">🔸 Visual Example (Simplified)</h2><p>Imagine a glossy red ball:</p><ul>  <li><strong>Without SH (SH=0):</strong> The ball is flat red from all directions.</li>  <li><strong>With SH=1:</strong> The ball appears brighter on the side facing the light source.</li>  <li><strong>With SH=2+:</strong> The ball has soft reflections and looks more realistic as you move the camera.</li></ul><h2 id=\"-dummy-python-code-example\">🔸 Dummy Python Code Example</h2><div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"kn\">import</span> <span class=\"nn\">numpy</span> <span class=\"k\">as</span> <span class=\"n\">np</span><span class=\"kn\">from</span> <span class=\"nn\">scipy.special</span> <span class=\"kn\">import</span> <span class=\"n\">sph_harm</span><span class=\"c1\"># Dummy view direction (theta, phi) in spherical coordinates</span><span class=\"n\">theta</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"p\">.</span><span class=\"n\">pi</span> <span class=\"o\">/</span> <span class=\"mi\">4</span>  <span class=\"c1\"># angle from z-axis</span><span class=\"n\">phi</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"p\">.</span><span class=\"n\">pi</span> <span class=\"o\">/</span> <span class=\"mi\">3</span>    <span class=\"c1\"># angle from x-axis in x-y plane</span><span class=\"c1\"># Compute SH basis up to degree l=2</span><span class=\"k\">def</span> <span class=\"nf\">compute_sh_basis</span><span class=\"p\">(</span><span class=\"n\">theta</span><span class=\"p\">,</span> <span class=\"n\">phi</span><span class=\"p\">,</span> <span class=\"n\">l_max</span><span class=\"o\">=</span><span class=\"mi\">2</span><span class=\"p\">):</span>    <span class=\"n\">basis</span> <span class=\"o\">=</span> <span class=\"p\">[]</span>    <span class=\"k\">for</span> <span class=\"n\">l</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"n\">l_max</span> <span class=\"o\">+</span> <span class=\"mi\">1</span><span class=\"p\">):</span>        <span class=\"k\">for</span> <span class=\"n\">m</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"o\">-</span><span class=\"n\">l</span><span class=\"p\">,</span> <span class=\"n\">l</span> <span class=\"o\">+</span> <span class=\"mi\">1</span><span class=\"p\">):</span>            <span class=\"n\">Y_lm</span> <span class=\"o\">=</span> <span class=\"n\">sph_harm</span><span class=\"p\">(</span><span class=\"n\">m</span><span class=\"p\">,</span> <span class=\"n\">l</span><span class=\"p\">,</span> <span class=\"n\">phi</span><span class=\"p\">,</span> <span class=\"n\">theta</span><span class=\"p\">).</span><span class=\"n\">real</span>  <span class=\"c1\"># Only real part used in graphics</span>            <span class=\"n\">basis</span><span class=\"p\">.</span><span class=\"n\">append</span><span class=\"p\">(</span><span class=\"n\">Y_lm</span><span class=\"p\">)</span>    <span class=\"k\">return</span> <span class=\"n\">np</span><span class=\"p\">.</span><span class=\"n\">array</span><span class=\"p\">(</span><span class=\"n\">basis</span><span class=\"p\">)</span><span class=\"c1\"># Example SH coefficients for RGB (assuming SH2 = 9 coeffs per channel)</span><span class=\"n\">sh_coeffs_r</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"p\">.</span><span class=\"n\">random</span><span class=\"p\">.</span><span class=\"n\">rand</span><span class=\"p\">(</span><span class=\"mi\">9</span><span class=\"p\">)</span><span class=\"n\">sh_coeffs_g</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"p\">.</span><span class=\"n\">random</span><span class=\"p\">.</span><span class=\"n\">rand</span><span class=\"p\">(</span><span class=\"mi\">9</span><span class=\"p\">)</span><span class=\"n\">sh_coeffs_b</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"p\">.</span><span class=\"n\">random</span><span class=\"p\">.</span><span class=\"n\">rand</span><span class=\"p\">(</span><span class=\"mi\">9</span><span class=\"p\">)</span><span class=\"c1\"># Evaluate SH basis</span><span class=\"n\">sh_basis</span> <span class=\"o\">=</span> <span class=\"n\">compute_sh_basis</span><span class=\"p\">(</span><span class=\"n\">theta</span><span class=\"p\">,</span> <span class=\"n\">phi</span><span class=\"p\">)</span><span class=\"c1\"># Final RGB color (dot product of SH coefficients and SH basis)</span><span class=\"n\">r</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"p\">.</span><span class=\"n\">dot</span><span class=\"p\">(</span><span class=\"n\">sh_coeffs_r</span><span class=\"p\">,</span> <span class=\"n\">sh_basis</span><span class=\"p\">)</span><span class=\"n\">g</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"p\">.</span><span class=\"n\">dot</span><span class=\"p\">(</span><span class=\"n\">sh_coeffs_g</span><span class=\"p\">,</span> <span class=\"n\">sh_basis</span><span class=\"p\">)</span><span class=\"n\">b</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"p\">.</span><span class=\"n\">dot</span><span class=\"p\">(</span><span class=\"n\">sh_coeffs_b</span><span class=\"p\">,</span> <span class=\"n\">sh_basis</span><span class=\"p\">)</span><span class=\"k\">print</span><span class=\"p\">(</span><span class=\"sa\">f</span><span class=\"s\">\"RGB color from SH: R=</span><span class=\"si\">{</span><span class=\"n\">r</span><span class=\"p\">:.</span><span class=\"mi\">2</span><span class=\"n\">f</span><span class=\"si\">}</span><span class=\"s\">, G=</span><span class=\"si\">{</span><span class=\"n\">g</span><span class=\"p\">:.</span><span class=\"mi\">2</span><span class=\"n\">f</span><span class=\"si\">}</span><span class=\"s\">, B=</span><span class=\"si\">{</span><span class=\"n\">b</span><span class=\"p\">:.</span><span class=\"mi\">2</span><span class=\"n\">f</span><span class=\"si\">}</span><span class=\"s\">\"</span><span class=\"p\">)</span></code></pre></div></div><p>This code shows:</p><ul>  <li>How SH basis functions are computed using <code class=\"language-plaintext highlighter-rouge\">scipy.special.sph_harm</code></li>  <li>How SH coefficients are applied to compute view-dependent color</li></ul><h2 id=\"-summary\">🔸 Summary</h2><p>In <strong>Gaussian Splatting</strong>, Spherical Harmonics:</p><ul>  <li>Encode how color/appearance changes with view direction</li>  <li>Provide a compact way to achieve realistic rendering</li>  <li>Are used per-splat (each Gaussian has its own SH coefficients)</li></ul><p>You can <strong>trade off quality vs speed</strong> by choosing the SH level (e.g., SH=0 for fast rendering, SH=2+ for better realism).</p>",
            "url": "http://localhost:4000/spherical-harmonics-in-gaussian-splats",
            
            
            
            
            
            "date_published": "2025-05-19T00:00:00+03:00",
            "date_modified": "2025-05-19T00:00:00+03:00",
            
                "author":  {
                "name": "Hüseyin Temiz",
                "url": null,
                "avatar": null
                }
                
            
        }
    
    ]
}